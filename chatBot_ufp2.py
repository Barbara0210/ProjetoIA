# Importa os m√≥dulos necess√°rios da biblioteca Hugging Face
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from datetime import datetime

# Define o ID do modelo que vais usar (modelo pr√©-treinado da Google chamado Gemma 2B IT)
model_id = "google/gemma-2b-it"
# Carrega o tokenizer (respons√°vel por converter texto em tokens num√©ricos)
tokenizer = AutoTokenizer.from_pretrained(model_id)
# Carrega o modelo de linguagem pr√©-treinado para tarefas de gera√ß√£o de texto
model = AutoModelForCausalLM.from_pretrained(model_id)

# Cria uma pipeline de gera√ß√£o de texto com o modelo e tokenizer carregados
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

#  Abre o ficheiro com exemplos (few-shot prompting)
# Este ficheiro deve conter exemplos reais de perguntas e respostas sobre a UFP
with open("exemplos_ufp_prompt.txt", "r", encoding="utf-8") as f:
    exemplos = f.read()

#  Cria o nome do ficheiro de log, com a data e hora atual
# Isto permite guardar um hist√≥rico da conversa automaticamente
log_filename = f"conversa_ufp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

print("‚úÖ Modelo pronto! Podes escrever perguntas sobre os cursos da UFP.")
print("‚úèÔ∏è Escreve 'sair' para terminar.\n")

#  Ciclo principal do chatbot (loop infinito at√© o utilizador escrever 'sair')
while True:
    pergunta = input("üì© Tu: ")
    if pergunta.lower() in ["sair", "exit", "quit"]:
        print("üëã At√© √† pr√≥xima!")
        break

    #  Cria o prompt que ser√° enviado ao modelo
    # Combina os exemplos com a nova pergunta    
    prompt = f"""{exemplos}

Pergunta: {pergunta}
Resposta:"""
    # Usa o modelo para gerar uma resposta com base no prompt
    # Limita a resposta a 150 tokens e permite amostragem para respostas mais variadas
    resposta = generator(prompt, max_new_tokens=150, do_sample=True)[0]["generated_text"]
    # Extrai a resposta final do texto gerado, removendo a parte do prompt
    resposta_final = resposta.split("Resposta:")[-1].strip()

    print("\nü§ñ Gemma:\n" + resposta_final + "\n")

    # Guarda pergunta e resposta no ficheiro de log
    with open(log_filename, "a", encoding="utf-8") as log_file:
        log_file.write(f"Pergunta: {pergunta}\n")
        log_file.write(f"Resposta: {resposta_final}\n\n")
